{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nsu4m5nStIiQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import triton.testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # Initialise Projection matrices\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        # Output Projection\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        # x is of shape [batch_size, seq_len, d_model]\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.W_q(x) # [batch_size, seq_len, d_model]\n",
        "        k = self.W_k(x) # [batch_size, seq_len, d_model]\n",
        "        v = self.W_v(x) # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Reshaping for multi-head attention\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, num_heads, head_dim]\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # Computing attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim) # [batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "        # Mask for causal attention\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        # [batch_size, num_heads, seq_len, seq_len]\n",
        "        attention_weights = F.softmax(scores, dim = -1)\n",
        "        # Softmax along dim = -1 to determine how much attention along each key dimension\n",
        "\n",
        "        # Applying dropout to attention weights\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Applying attention weights to valyues\n",
        "        # [batch_size, num_heads, seq_len, head_dim]\n",
        "        context = torch.matmul(attention_weights, v)\n",
        "\n",
        "        # Concatenating heads\n",
        "        # [batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
        "        context = context.transpose(1, 2)\n",
        "        # [batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, d_model]\n",
        "        context = context.contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "Xr6v5QDytthI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating causal mask"
      ],
      "metadata": {
        "id": "St7uwCO6xwyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(seq_len):\n",
        "    # Lower tringular matrix so that query tokens dont have access to keys that come after them in the sequence\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len)))\n",
        "\n",
        "    return mask.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, seq_len]"
      ],
      "metadata": {
        "id": "hBCnPJjuxrWC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark_and_analyze()"
      ],
      "metadata": {
        "id": "9lq4FEQ5yts6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive triton implementation"
      ],
      "metadata": {
        "id": "hL7qtNXFCHBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def attention_kernel(\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    batch_size, seq_len, num_heads, head_dim,\n",
        "    q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "    k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "    v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "    o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "    scale, # 1 / square_root(d_k)\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "    ):\n",
        "\n",
        "    batch_idx = tl.program_id(0)\n",
        "    head_idx = tl.program_id(1)\n",
        "    seq_idx =  tl.program_id(2)\n",
        "\n",
        "    # Computing pointer offsets\n",
        "    # Navigate to correct starting positions\n",
        "    q_batch_offset = batch_idx * q_batch_stride\n",
        "    q_head_offset = head_idx * q_head_stride\n",
        "    q_seq_offset = seq_idx * q_seq_stride\n",
        "\n",
        "    # No sequence offset for K and V since each query block will see all the key blocks\n",
        "    k_batch_offset = batch_idx * k_batch_stride\n",
        "    k_head_offset = head_idx * k_head_stride\n",
        "\n",
        "    v_batch_offset = batch_idx * v_batch_stride\n",
        "    v_head_offset = head_idx * v_head_stride\n",
        "\n",
        "    o_batch_offset = batch_idx * o_batch_stride\n",
        "    o_head_offset = head_idx * o_head_stride\n",
        "    o_seq_offset = seq_idx * o_seq_stride\n",
        "\n",
        "    # Loading query vector for this sequence position\n",
        "    q_ptrs = q_ptr + q_batch_offset + q_head_offset + q_seq_offset + tl.arange(0, BLOCK_SIZE) * q_head_dim_stride # This loads data even if it is in non contiguous locations in memory\n",
        "    q = tl.load(q_ptrs, mask = tl.arange(0, BLOCK_SIZE) < head_dim, other = 0.0)\n",
        "\n",
        "    # Initialise accumulator for weighted sum. One score for each key token\n",
        "    acc = tl.zeros([BLOCK_SIZE], dtype = tl.float32)\n",
        "\n",
        "    softmax_denominator = 0.0\n",
        "\n",
        "    for k_seq_idx in range(seq_len):\n",
        "        k_seq_offset = k_seq_idx * k_seq_stride\n",
        "        k_ptrs =  k_ptr + k_batch_offset + k_head_offset + k_seq_offset + tl.arange(0, BLOCK_SIZE) * k_head_dim_stride\n",
        "        k = tl.load(k_ptrs, mask = tl.arange(0, BLOCK_SIZE) < head_dim, other = 0.0)\n",
        "\n",
        "        score = tl.sum(q * k) * scale\n",
        "        attention_weight = tl.exp(score)\n",
        "\n",
        "        softmax_denominator += attention_weight\n",
        "\n",
        "        v_seq_offset = k_seq_idx * v_seq_stride\n",
        "        v_ptrs = v_ptr + v_batch_offset + v_head_offset + v_seq_offset + tl.arange(0, BLOCK_SIZE) * v_head_dim_stride\n",
        "\n",
        "        v = tl.load(v_ptrs, mask = tl.arange(0, BLOCK_SIZE) < head_dim, other = 0.0)\n",
        "\n",
        "        acc += attention_weight * v\n",
        "\n",
        "    acc /= softmax_denominator\n",
        "\n",
        "    output_ptrs = o_ptr + o_batch_offset + o_head_offset + o_seq_offset + tl.arange(0, BLOCK_SIZE) * o_head_dim_stride\n",
        "    tl.store(output_ptrs, acc, mask = tl.arange(0, BLOCK_SIZE) < head_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "f4feaH8uyvsy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttentionNaive(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        o = torch.empty_like(q)\n",
        "\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        grid = (batch_size, self.num_heads, seq_len)\n",
        "\n",
        "        block_size = 1\n",
        "        while block_size < self.head_dim:\n",
        "            block_size *= 2\n",
        "\n",
        "        attention_kernel[grid](\n",
        "            q, k, v, o,\n",
        "            batch_size, seq_len, self.num_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE = block_size\n",
        "        )\n",
        "\n",
        "        #  [batch_size, num_heads , seq_len, head_dim] -> [batch_size, seq_len, num_heads, head_dim] -> [batch_size, seq_len, d_model]\n",
        "        o = o.permute(0,2,1,3).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        o = self.W_o(o)\n",
        "\n",
        "        return o"
      ],
      "metadata": {
        "id": "4_FypGRhKEgW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized Multi Head Attention using Triton"
      ],
      "metadata": {
        "id": "gj-F-iWTW8Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def optimized_attention_kernel(\n",
        "      q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "      batch_size, seq_len, num_heads, head_dim,\n",
        "      q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "      k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "      v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "      o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "      scale, # 1 / square_root(d_k)\n",
        "      BLOCK_SIZE_M: tl.constexpr, # Query Block Size\n",
        "      BLOCK_SIZE_N: tl.constexpr, # Key Block Size\n",
        "      BLOCK_SIZE_DMODEL : tl.constexpr, # Head Dimension Block Size,\n",
        "      USE_CAUSAL_MASK : tl.constexpr\n",
        "      ):\n",
        "\n",
        "      batch_id = tl.program_id(0)\n",
        "      head_id = tl.program_id(1)\n",
        "      seq_start =  tl.program_id(2) * BLOCK_SIZE_M\n",
        "\n",
        "      q_head_offset = head_id * q_head_stride\n",
        "      k_head_offset = head_id * k_head_stride\n",
        "      v_head_offset = head_id * v_head_stride\n",
        "\n",
        "      q_batch_offset = batch_id * q_batch_stride\n",
        "      k_batch_offset = batch_id * k_batch_stride\n",
        "      v_batch_offset = batch_id * v_batch_stride\n",
        "\n",
        "      o_head_offset = head_id * o_head_stride\n",
        "      o_batch_offset = batch_id * o_batch_stride\n",
        "\n",
        "      # Initializing accumulators\n",
        "      m_i = tl.zeros([BLOCK_SIZE_M], dtype = tl.float32) - float('inf')\n",
        "      l_i = tl.zeros([BLOCK_SIZE_M], dtype = tl.float32)\n",
        "      acc = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_DMODEL], dtype = tl.float32)\n",
        "\n",
        "      q_block_mask = (seq_start + tl.arange(0, BLOCK_SIZE_M)) < seq_len\n",
        "\n",
        "      # Processing key blocks\n",
        "      for key_start in range(0, seq_len, BLOCK_SIZE_N):\n",
        "          k_block_mask = (key_start + tl.arange(0, BLOCK_SIZE_N)) < seq_len\n",
        "\n",
        "          if USE_CAUSAL_MASK:\n",
        "            causal_mask = tl.arange(0, BLOCK_SIZE_M)[:, None] + seq_start >= tl.arange(0, BLOCK_SIZE_N)[None, :] + key_start # Process only tokens that occur before the given query\n",
        "\n",
        "          # Loading Query Block [BLOCK_M, BLOCK_DMODEL]\n",
        "          q_block_ptr = q_ptr + q_batch_offset + q_head_offset + (seq_start + tl.arange(0, BLOCK_SIZE_M)[:, None])* q_seq_stride + (key_start + tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * q_head_dim_stride\n",
        "\n",
        "          q_block = tl.load(q_block_ptr, mask=q_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim), other=0.0)\n",
        "\n",
        "          # Loading Key Block [BLOCK_N, BLOCK_DMODEL]\n",
        "          k_block_ptr = k_ptr + k_batch_offset + k_head_offset + (key_start + tl.arange(0,BLOCK_SIZE_N)[:, None])* k_seq_stride + (key_start + tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * k_head_dim_stride\n",
        "\n",
        "          k_block = tl.load(k_block_ptr, mask=k_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim), other=0.0)\n",
        "\n",
        "          # Computing attention scores\n",
        "          scores = tl.dot(q_block, tl.trans(k_block)) * scale\n",
        "\n",
        "          if USE_CAUSAL_MASK:\n",
        "            scores = tl.where(causal_mask, scores, float(\"-inf\")) # This will be zeroed our during softmax\n",
        "\n",
        "          # Stable Softmax Computation\n",
        "          # 1. Computing Max for Numerical Stability\n",
        "          m_ij = tl.max(scores, axis = 1)\n",
        "\n",
        "          # 2. Updating Running Max\n",
        "          m_i_new = tl.maximum(m_i, m_ij)\n",
        "\n",
        "          # 3. Computing Exponentials with the updated max\n",
        "          exp_scores = tl.exp(scores - m_i_new[:, None])\n",
        "\n",
        "          # 4. Compute Scaling factor for previous computations\n",
        "          alpha = tl.exp(m_i - m_i_new)\n",
        "\n",
        "          # 5. Updating normalization factor\n",
        "          l_i_new = alpha * l_i + tl.sum(exp_scores, axis = 1)\n",
        "\n",
        "          # Loading Value block [BLOCK_N, BLOCK_DMODEL]\n",
        "          v_block_ptr = v_ptr + v_batch_offset + v_head_offset + (key_start + tl.arange(0, BLOCK_SIZE_N)[:, None])* v_seq_stride + (key_start + tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * v_head_dim_stride\n",
        "\n",
        "          v_block = tl.load(v_block_ptr, mask=k_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim), other=0.0)\n",
        "\n",
        "          acc = acc * alpha[:, None] + tl.dot(exp_scores, v_block)\n",
        "\n",
        "          m_i = m_i_new\n",
        "          l_i = l_i_new\n",
        "\n",
        "      acc /= l_i[:, None]\n",
        "\n",
        "      o_block_ptr = o_ptr + o_batch_offset + o_head_offset + (seq_start + tl.arange(0, BLOCK_SIZE_M)[:, None])* o_seq_stride + (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :]) * o_head_dim_stride\n",
        "\n",
        "      tl.store(o_block_ptr, acc, mask=q_block_mask[:, None] & (tl.arange(0, BLOCK_SIZE_DMODEL)[None, :] < head_dim))"
      ],
      "metadata": {
        "id": "qngNX2yKXAPs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttentionOptimized(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, causal = False):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.causal = causal\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k =nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Prepare output tensor\n",
        "        output = torch.empty_like(q)\n",
        "\n",
        "        # Scaling factor\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        BLOCK_M = 16\n",
        "        BLOCK_N = 16\n",
        "\n",
        "        # Round head_dim up to the nearest power of 2 for BLOCK_DMODEL\n",
        "        BLOCK_DMODEL = 1\n",
        "        while BLOCK_DMODEL < self.head_dim:\n",
        "            BLOCK_DMODEL *= 2\n",
        "\n",
        "        grid = (batch_size, self.num_heads, triton.cdiv(seq_len, BLOCK_M))\n",
        "\n",
        "        optimized_attention_kernel[grid](\n",
        "            q, k, v, output,\n",
        "            batch_size, seq_len, self.num_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE_M=BLOCK_M,\n",
        "            BLOCK_SIZE_N=BLOCK_N,\n",
        "            BLOCK_SIZE_DMODEL=BLOCK_DMODEL,\n",
        "            USE_CAUSAL_MASK=self.causal,\n",
        "        )\n",
        "\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "uNaoytxFewMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flash Attention v2"
      ],
      "metadata": {
        "id": "Q9a_PlYWz-eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have hierarchial tiling, the blocks are further divided and processed as sub-blocks"
      ],
      "metadata": {
        "id": "7FZam6H602xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def flash_attn_v2_forward(\n",
        "    # Pointers to matrices\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    # Matrix dimensions\n",
        "    batch_size, seq_len, num_heads, head_dim,\n",
        "    # Strides for accessing tensors\n",
        "    q_batch_stride, q_head_stride, q_seq_stride, q_head_dim_stride,\n",
        "    k_batch_stride, k_head_stride, k_seq_stride, k_head_dim_stride,\n",
        "    v_batch_stride, v_head_stride, v_seq_stride, v_head_dim_stride,\n",
        "    o_batch_stride, o_head_stride, o_seq_stride, o_head_dim_stride,\n",
        "    # Scale factor\n",
        "    scale,\n",
        "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "    # Causal flag\n",
        "    IS_CAUSAL: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simplified Flash Attention V2 forward pass\n",
        "    \"\"\"\n",
        "    # Program ID\n",
        "    batch_id = tl.program_id(0)\n",
        "    head_id = tl.program_id(1)\n",
        "    m_id = tl.program_id(2)\n",
        "\n",
        "    # Starting row index\n",
        "    start_m = m_id * BLOCK_SIZE_M\n",
        "\n",
        "    q_batch_offset = batch_id * q_batch_stride\n",
        "    q_head_offset = head_id * q_head_stride\n",
        "    k_batch_offset = batch_id * k_batch_stride\n",
        "    k_head_offset = head_id * k_head_stride\n",
        "    v_batch_offset = batch_id * v_batch_stride\n",
        "    v_head_offset = head_id * v_head_stride\n",
        "    o_batch_offset = batch_id * o_batch_stride\n",
        "    o_head_offset = head_id * o_head_stride\n",
        "\n",
        "    # Initialize accumulators\n",
        "    m_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32) - float(\"inf\")\n",
        "    l_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32)\n",
        "    acc = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_K], dtype=tl.float32)\n",
        "\n",
        "    # Create row indices and mask\n",
        "    row_indices = start_m + tl.arange(0, BLOCK_SIZE_M)\n",
        "    row_mask = row_indices < seq_len\n",
        "\n",
        "    # Loading Q block once - I'll reuse it for all K,V blocks\n",
        "    q_block = tl.load(\n",
        "        q_ptr + q_batch_offset + q_head_offset +\n",
        "        row_indices[:, None] * q_seq_stride +\n",
        "        tl.arange(0, BLOCK_SIZE_K)[None, :] * q_head_dim_stride,\n",
        "        mask=row_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "        other=0.0\n",
        "    )\n",
        "\n",
        "    # Process blocks of K and V\n",
        "    for start_n in range(0, seq_len, BLOCK_SIZE_N):\n",
        "        col_indices = start_n + tl.arange(0, BLOCK_SIZE_N)\n",
        "        col_mask = col_indices < seq_len\n",
        "\n",
        "        if IS_CAUSAL:\n",
        "            causal_mask = row_indices[:, None] >= col_indices[None, :]\n",
        "\n",
        "        # Loading K block\n",
        "        k_block = tl.load(\n",
        "            k_ptr + k_batch_offset + k_head_offset +\n",
        "            col_indices[:, None] * k_seq_stride +\n",
        "            tl.arange(0, BLOCK_SIZE_K)[None, :] * k_head_dim_stride,\n",
        "            mask=col_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "            other=0.0\n",
        "        )\n",
        "\n",
        "        scores = tl.dot(q_block, tl.trans(k_block)) * scale\n",
        "\n",
        "        if IS_CAUSAL:\n",
        "            scores = tl.where(causal_mask, scores, float('-inf'))\n",
        "\n",
        "        # Compute new max for stable softmax\n",
        "        m_i_new = tl.maximum(m_i, tl.max(scores, axis=1))\n",
        "        alpha = tl.exp(m_i - m_i_new)\n",
        "\n",
        "        # Update max values\n",
        "        m_i = m_i_new\n",
        "\n",
        "        # Compute softmax values with updated max\n",
        "        p = tl.exp(scores - m_i[:, None])\n",
        "\n",
        "        # Load V block\n",
        "        v_block = tl.load(\n",
        "            v_ptr + v_batch_offset + v_head_offset +\n",
        "            col_indices[:, None] * v_seq_stride +\n",
        "            tl.arange(0, BLOCK_SIZE_K)[None, :] * v_head_dim_stride,\n",
        "            mask=col_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim),\n",
        "            other=0.0\n",
        "        )\n",
        "\n",
        "        # Update sum of exponentials\n",
        "        l_i_new = alpha * l_i + tl.sum(p, axis=1)\n",
        "\n",
        "        # Update weighted sum\n",
        "        acc_new = alpha[:, None] * acc + tl.dot(p, v_block)\n",
        "\n",
        "        # Update accumulators\n",
        "        l_i = l_i_new\n",
        "        acc = acc_new\n",
        "\n",
        "    # Normalize output\n",
        "    out = acc / l_i[:, None]\n",
        "\n",
        "    # Store output\n",
        "    tl.store(\n",
        "        o_ptr + o_batch_offset + o_head_offset +\n",
        "        row_indices[:, None] * o_seq_stride +\n",
        "        tl.arange(0, BLOCK_SIZE_K)[None, :] * o_head_dim_stride,\n",
        "        out,\n",
        "        mask=row_mask[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < head_dim)\n",
        "    )"
      ],
      "metadata": {
        "id": "HapgJ6AkkP1B"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlashAttentionV2(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        causal=False,\n",
        "        block_size=64\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.dropout = dropout\n",
        "        self.causal = causal\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Linear projections\n",
        "        self.W_q = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = torch.nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Make sure head_dim is a power of 2\n",
        "        if not (self.head_dim & (self.head_dim - 1) == 0):\n",
        "            raise ValueError(f\"Head dimension ({self.head_dim}) must be a power of 2\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x)\n",
        "        v = self.W_v(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Prepare output tensor\n",
        "        output = torch.empty_like(q)\n",
        "\n",
        "        # Scaling factor\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Calculate grid dimensions\n",
        "        grid = (\n",
        "            batch_size,\n",
        "            self.num_heads,\n",
        "            triton.cdiv(seq_len, self.block_size)\n",
        "        )\n",
        "\n",
        "        # Round head_dim up to the nearest power of 2 if needed\n",
        "        block_k = self.head_dim\n",
        "        if block_k & (block_k - 1) != 0:\n",
        "            block_k = 1\n",
        "            while block_k < self.head_dim:\n",
        "                block_k *= 2\n",
        "\n",
        "        # Launch kernel\n",
        "        flash_attn_v2_forward[grid](\n",
        "            q, k, v, output,\n",
        "            batch_size, seq_len, self.num_heads, self.head_dim,\n",
        "            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
        "            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
        "            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
        "            output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n",
        "            scale,\n",
        "            BLOCK_SIZE_M=self.block_size,\n",
        "            BLOCK_SIZE_N=self.block_size,\n",
        "            BLOCK_SIZE_K=block_k,\n",
        "            IS_CAUSAL=self.causal,\n",
        "        )\n",
        "\n",
        "        # Reshape output back\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # Apply dropout\n",
        "        if self.dropout > 0.0 and self.training:\n",
        "            output = torch.nn.functional.dropout(output, p=self.dropout)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "o-nFY-i3kWL1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flash-attn"
      ],
      "metadata": {
        "id": "5Y0pNe9sdRDO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    FLASH_ATTN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"flash-attn library not found. Will skip that implementation.\")\n",
        "    FLASH_ATTN_AVAILABLE = False"
      ],
      "metadata": {
        "id": "kSH2dRKgdYbl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "DEVICE = torch.device('cuda')\n",
        "D_MODEL = 768\n",
        "NUM_HEADS = 12\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# flash-attn library typically uses these block sizes\n",
        "FLASH_BLOCK_SIZE = 64\n",
        "FLASH_SUB_BLOCK_SIZE = 16"
      ],
      "metadata": {
        "id": "0m46mOjReAcn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.testing.perf_report(\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=['seq_len'],\n",
        "        x_vals=[2**i for i in range(8, 13)],  # 256 to 4096\n",
        "        x_log=True,\n",
        "        line_arg='provider',\n",
        "        line_vals=['torch', 'triton_naive', 'triton_optimized', 'flash_v2', 'flash_attn_lib'],\n",
        "        # line_vals=['torch', 'triton_naive', 'triton_optimized', 'flash_v2'],\n",
        "        line_names=['PyTorch', 'Triton Naive', 'Triton Optimized', 'My Flash V2', 'Flash-Attn Lib'],\n",
        "        # line_names=['PyTorch', 'Triton Naive', 'Triton Optimized', 'My Flash V2'],\n",
        "        styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('purple', '-'), ('orange', '-')],\n",
        "        # styles=[('blue', '-'), ('green', '-'), ('red', '-'), ('purple', '-')],\n",
        "        # ylabel='Latency (ms)',\n",
        "        plot_name='attention-performance',\n",
        "        args={'d_model': D_MODEL, 'num_heads': NUM_HEADS, 'batch_size': BATCH_SIZE},\n",
        "    )\n",
        ")\n",
        "def benchmark_attention(seq_len, provider, d_model, num_heads, batch_size):\n",
        "    \"\"\"Benchmark different attention implementations.\"\"\"\n",
        "    x = torch.randn((batch_size, seq_len, d_model), device=DEVICE)\n",
        "\n",
        "    # Create the appropriate model based on provider\n",
        "    if provider == 'torch':\n",
        "        model = TorchAttention(d_model, num_heads).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'triton_naive':\n",
        "        model = TritonAttentionNaive(d_model, num_heads).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'triton_optimized':\n",
        "        model = TritonAttentionOptimized(d_model, num_heads, causal=False).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'flash_v2':\n",
        "        model = FlashAttentionV2(\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            block_size=FLASH_BLOCK_SIZE\n",
        "        ).to(DEVICE)\n",
        "        run_func = lambda: model(x)\n",
        "\n",
        "    elif provider == 'flash_attn_lib':\n",
        "        if not FLASH_ATTN_AVAILABLE:\n",
        "            return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "        # For the flash-attn library, prepare inputs differently\n",
        "        q_proj = torch.nn.Linear(d_model, d_model, bias=False).to(DEVICE)\n",
        "        k_proj = torch.nn.Linear(d_model, d_model, bias=False).to(DEVICE)\n",
        "        v_proj = torch.nn.Linear(d_model, d_model, bias=False).to(DEVICE)\n",
        "\n",
        "        q = q_proj(x).view(batch_size, seq_len, num_heads, d_model // num_heads)\n",
        "        k = k_proj(x).view(batch_size, seq_len, num_heads, d_model // num_heads)\n",
        "        v = v_proj(x).view(batch_size, seq_len, num_heads, d_model // num_heads)\n",
        "\n",
        "        q = q.to(torch.float16)\n",
        "        k = k.to(torch.float16)\n",
        "        v = v.to(torch.float16)\n",
        "\n",
        "        run_func = lambda: flash_attn_func(q, k, v, causal=False)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider}\")\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(3):\n",
        "        _ = run_func()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Benchmark\n",
        "    quantiles = [0.5, 0.2, 0.8]\n",
        "    ms, min_ms, max_ms = triton.testing.do_bench(run_func, quantiles=quantiles)\n",
        "\n",
        "    return ms, max_ms, min_ms"
      ],
      "metadata": {
        "id": "K36w3smghgqQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_attention.run(save_path='./attention_benchmark', print_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "OAnnBtAJi9Pn",
        "outputId": "9f26387a-a70c-4c5c-ccae-b365901fb27f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention-performance:\n",
            "   seq_len    PyTorch  Triton Naive  Triton Optimized  My Flash V2  Flash-Attn Lib\n",
            "0    256.0   0.267264      1.033728          0.223232     0.201728        0.024576\n",
            "1    512.0   0.580576      3.821568          0.395776     0.314368        0.047104\n",
            "2   1024.0   2.371584     14.738944          0.983552     0.666624        0.090112\n",
            "3   2048.0   8.394753     58.597378          3.210240     1.766912        0.259584\n",
            "4   4096.0  31.447041    231.646210         10.640385     4.966400        0.864256\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAYu-cSvlof8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}